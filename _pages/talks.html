---
layout: archive
title: "Talks and presentations"
permalink: /talks/
author_profile: true
---


---
title: "CC-Cert: A Probabilistic Approach to Certify General Robustness of Neural Networks"
collection: talks
type: "Conference talk"
permalink: # /talks/2022_cccert/
venue: "AAAI-2022"
date: 2022-Feb-26
location: "Virtual conference"
---
In safety-critical machine learning applications, it is crucial to defend models against adversarial attacks --- small modifications of the input that change the predictions. Besides rigorously studied $\ell_p$-bounded additive perturbations, semantic perturbations (e.g. rotation, translation) raise a serious concern on deploying ML systems in real-world. Therefore, it is important to provide provable guarantees for deep learning models against semantically meaningful input transformations. In this paper, we propose a new universal probabilistic certification approach based on Chernoff-Cramer bounds that can be used in general attack settings. We estimate the probability of a model to fail if the attack is sampled from a certain distribution. Our theoretical findings are supported by experimental results on different datasets.

[Online recording](https://aaai-2022.virtualchair.net/poster_aaai10699)


---
title: "Smoothed Embeddings for Certified Few-Shot Learning"
collection: talks
type: "Conference talk"
permalink: # /talks/2022_smoothed_emb/
venue: "ISP RAS open conference"
date: 2022-Dec-02
location: "Moscow, Russia"
---

Randomized smoothing is considered to be the state-of-the-art provable defense against adversarial perturbations. However, it heavily exploits the fact that classifiers map input objects to class probabilities and do not focus on the ones that learn a metric space in which classification is performed by computing distances to embeddings of classes prototypes. In this work, we extend randomized smoothing to few-shot learning models that map inputs to normalized embeddings. We provide analysis of Lipschitz continuity of such models and derive robustness certificate against $l_2$-bounded perturbations that may be useful in few-shot learning scenarios. Our theoretical results are confirmed by experiments on different datasets.

I gave a talk about the [NeurIPS paper](https://openreview.net/forum?id=m2JJO3iEe_5) on the second day. [Here is the conference site](https://www.isprasopen.ru/)